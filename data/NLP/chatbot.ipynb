{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain: Chat with your data",
   "id": "1ef6e93e72c6e8e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LangChain is open-source developer framework for building LLM applications.  \n",
    "LLM as we know it knows only the data that it was trained on, but it would be useful if we could talk with LLM, and if he knew the data that we provided previously. This is exactly the topic we will deal with by playing with LangChain framework."
   ],
   "id": "ad0ab31d54a034d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieval-Augmented Generation (RAG) with LangChain\n",
    "\n",
    " The RAG system retrieves relevant information from a predefined knowledge base and then uses a generative language model to make a response based on that retrieved information.  \n",
    " The idea is to ground the model‚Äôs responses in specific information, reducing the risk of generating hallucinated answers. It‚Äôs particularly useful for applications requiring factual accuracy or answers based on domain-specific knowledge.  \n",
    " There are two main steps:  \n",
    "  - <b>Retrieval:</b> The system searches a database or document collection for relevant pieces of information based on the query. \n",
    "  - <b>Generation:</b> A language model takes the retrieved information and makes a contextually relevant response.\n",
    " \n",
    "LangChain is a framework designed for chaining together various components, such as retrieval systems and language models, to build RAG systems. LangChain modules support:  \n",
    "- Document Loading and Preprocessing\n",
    "- Vector Store Integration and efficient scaling - as the knowledge base grows, LangChain‚Äôs vector store and retrieval functions help maintain efficient query processing times\n",
    "- Retrieval-Generation Chains -  provide Domain-Specific Knowledge and reduce hallucination  \n",
    "  \n",
    "We are going to start with document loading.\n"
   ],
   "id": "68c70141294a00e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Document Loading\n",
    "#### PDF loading"
   ],
   "id": "e24f2ebebbc24d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:52:31.989785Z",
     "start_time": "2024-10-27T14:52:31.653473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"knowledge_base/LinearRegression-Lecture01.pdf\")\n",
    "pages = loader.load()"
   ],
   "id": "3e10ef567df573ca",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- PyPDFLoader loaded a list of documents and each document is for each page in the pdf\n",
    "- Each document has its page_content and metadata (source destination and page number)\n",
    "- Extract images, lazy load option"
   ],
   "id": "8360c2443078ec34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:49:09.305517Z",
     "start_time": "2024-10-27T14:49:09.296547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Loaded document has {len(pages)} pages.\")\n",
    "last_page = pages[-1]\n",
    "print(f\"Last page metadata: {last_page.metadata}\")\n",
    "print(last_page.page_content[-602:-2]) # print out the document summary"
   ],
   "id": "194f08e012a5128f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded document has 13 pages.\n",
      "Last page metadata: {'source': 'knowledge_base/LinearRegression-Lecture01.pdf', 'page': 12}\n",
      "\n",
      "Summary\n",
      "‚Ä¢Regression is used to predict numeric values (dependent variables) based on input exam-\n",
      "ples (independent variables)\n",
      "‚Ä¢Inlinear regression , the output value is a linear combination of input values\n",
      "‚Ä¢The loss function is the quadratic loss (the squared diÔ¨Äerence between the predicted and\n",
      "correct label)\n",
      "‚Ä¢Optimization uses the least squares method , computed using the pseudoinverse of the\n",
      "design matrix\n",
      "‚Ä¢Assuming normally distributed noise, the least squares solution is equivalent to maximizing\n",
      "the probability of the labels, which gives a probabilistic justiÔ¨Åcation for the quadratic loss\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### URL loading\n",
    "We can use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. "
   ],
   "id": "517cab18b0d4eae4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:49:14.501093Z",
     "start_time": "2024-10-27T14:49:12.095937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/introduction/\")\n",
    "docs = loader.load()\n",
    "print(f\"METADATA: {docs[0].metadata}\\n\")\n",
    "content = docs[0].page_content\n",
    "# Print out a part of the page to explain langchain\n",
    "start = content.find(\"LangChain is a framework for developing applications\")\n",
    "end = content.find(\"LangGraph Cloud.\") + len(\"LangGraph Cloud.\")\n",
    "print(content[start:end])\n",
    "\n",
    "# print(docs[0].page_content[:])   # print the whole page"
   ],
   "id": "b1301dcf05cfecbf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METADATA: {'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}\n",
      "\n",
      "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
      "LangChain simplifies every stage of the LLM application lifecycle:\n",
      "\n",
      "Development: Build your applications using LangChain's open-source building blocks, components, and third-party integrations.\n",
      "Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
      "Productionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.\n",
      "Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load multiple URLs concurrently  \n",
    "We can speed up the scraping process by scraping and parsing multiple URLs concurrently.  \n",
    "Parameter <b>requests_per_second</b> is used to increase the max concurrent requests. This will speed up the scraping process, but may cause the server to block you.\n"
   ],
   "id": "8b606fbff592851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:05:21.504294Z",
     "start_time": "2024-10-27T10:05:20.810689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nest_asyncio # fixes a bug with asyncio and jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "loader = WebBaseLoader([\"https://python.langchain.com/docs/introduction/\", \"https://huggingface.co/docs/transformers/index\"])\n",
    "loader.requests_per_second = 1 \n",
    "docs = loader.aload()\n",
    "for doc in docs:\n",
    "    print(f\"{doc.metadata}\\n\")\n"
   ],
   "id": "c33e8ea7a9d789d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 2/2 [00:00<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}\n",
      "\n",
      "{'source': 'https://huggingface.co/docs/transformers/index', 'title': 'ü§ó Transformers', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Loading a xml file  \n",
    "Load a XML file using RAW URL and default parser set to 'xml' (install lxml)"
   ],
   "id": "f2062c1d502a2c9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:49:28.876029Z",
     "start_time": "2024-10-27T14:49:28.577050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = WebBaseLoader(\n",
    "    \"https://raw.githubusercontent.com/IvaGoluza/backend_RoundRobin/refs/heads/master/pom.xml\"\n",
    ")\n",
    "loader.default_parser = \"xml\"\n",
    "docs = loader.load()\n",
    "print(f\"METADATA: {docs[0].metadata}\\n\")\n",
    "docs[0].page_content"
   ],
   "id": "734b175e33a0fcba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METADATA: {'source': 'https://raw.githubusercontent.com/IvaGoluza/backend_RoundRobin/refs/heads/master/pom.xml'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n4.0.0\\n\\norg.springframework.boot\\nspring-boot-starter-parent\\n3.1.5\\n \\n\\ncom.web2\\nRoundRobin\\n0.0.1-SNAPSHOT\\nRoundRobin\\nDemo project for Spring Boot\\n\\n17\\n3.1.0\\n\\n\\n\\norg.springframework.boot\\nspring-boot-starter-data-jpa\\n\\n\\norg.springframework.boot\\nspring-boot-starter-security\\n\\n\\norg.springframework.boot\\nspring-boot-starter-thymeleaf\\n\\n\\norg.springframework.boot\\nspring-boot-starter-web\\n\\n\\norg.postgresql\\npostgresql\\nruntime\\n\\n\\norg.springframework.boot\\nspring-boot-configuration-processor\\ntrue\\n\\n\\norg.springframework.boot\\nspring-boot-starter-validation\\n\\n\\norg.projectlombok\\nlombok\\ntrue\\n\\n\\norg.modelmapper\\nmodelmapper\\n${modelmapper.version}\\n\\n\\norg.springframework.boot\\nspring-boot-starter-test\\ntest\\n\\n\\norg.springframework.security\\nspring-security-test\\ntest\\n\\n\\n\\n\\n\\norg.springframework.boot\\nspring-boot-maven-plugin\\n\\n\\n\\norg.projectlombok\\nlombok\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Lazy Load  \n",
    "We can use lazy load with the previous loader (only for demo, but this feature is great for loading large data in order to minimize memory requirements). "
   ],
   "id": "3abbaec0953c4cca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:05:33.000338Z",
     "start_time": "2024-10-27T10:05:32.972021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pages = []\n",
    "for doc in loader.lazy_load():\n",
    "    pages.append(doc)\n",
    "\n",
    "print(pages[0].page_content[:])\n",
    "print(f\"METADATA: {pages[0].metadata}\\n\")"
   ],
   "id": "6af2c881326a909b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.0.0\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-starter-parent\n",
      "3.1.5\n",
      " \n",
      "\n",
      "com.web2\n",
      "RoundRobin\n",
      "0.0.1-SNAPSHOT\n",
      "RoundRobin\n",
      "Demo project for Spring Boot\n",
      "\n",
      "17\n",
      "3.1.0\n",
      "\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-starter-data-jpa\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-starter-security\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-starter-thymeleaf\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-starter-web\n",
      "\n",
      "\n",
      "org.postgresql\n",
      "postgresql\n",
      "runtime\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-configuration-processor\n",
      "true\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-starter-validation\n",
      "\n",
      "\n",
      "org.projectlombok\n",
      "lombok\n",
      "true\n",
      "\n",
      "\n",
      "org.modelmapper\n",
      "modelmapper\n",
      "${modelmapper.version}\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-starter-test\n",
      "test\n",
      "\n",
      "\n",
      "org.springframework.security\n",
      "spring-security-test\n",
      "test\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "org.springframework.boot\n",
      "spring-boot-maven-plugin\n",
      "\n",
      "\n",
      "\n",
      "org.projectlombok\n",
      "lombok\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "METADATA: {'source': 'https://raw.githubusercontent.com/IvaGoluza/backend_RoundRobin/refs/heads/master/pom.xml'}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Loaded documents are large so now we should split them in smaller chunks.\n",
    "### Document Splitting  \n",
    "- We are splitting documents in semantic chunks. Size is measured in characters or tokens.  \n",
    "- The basis of all text splitters in LangChain involve splitting with some chunk overlap. This way chunks are connected.  \n",
    "- Text splitters in LangChain all have <b>create_documents</b> and <b>split_documents</b> method.  \n",
    "- Some text splitters are focused on metadata. When the document is split in chunks, every chunk has to have initial metadata, and maybe some additional metadata.\n"
   ],
   "id": "d6b292218517c7e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:52:49.361950Z",
     "start_time": "2024-10-27T14:52:49.323579Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter",
   "id": "8f6d6dde393fb9b0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:49:41.729717Z",
     "start_time": "2024-10-27T14:49:41.723123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunk_size = 30\n",
    "chunk_overlap = 10\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ],
   "id": "c9f65d46b4da3ef9",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:05:41.356776Z",
     "start_time": "2024-10-27T10:05:41.344520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = 'Today is Tuesday and we are in Natural Language Processing class.'\n",
    "print(f\"Character text splitter:\\n {c_splitter.split_text(text)}\\n\\n\")\n",
    "print(f\"Recursive Character text splitter:\\n {r_splitter.split_text(text)}\\n\\n\")"
   ],
   "id": "cd9317cdab14ba91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character text splitter:\n",
      " ['Today is Tuesday and we are in Natural Language Processing class.']\n",
      "\n",
      "\n",
      "Recursive Character text splitter:\n",
      " ['Today is Tuesday and we are in', 'we are in Natural Language', 'Language Processing class.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By default, character splitter is using new line as a separator, and doesn't even split the previous string until we set its separator to ''.",
   "id": "b86b76d0086e357d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:05:45.475013Z",
     "start_time": "2024-10-27T10:05:45.467262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Character splitter using default separator:\\n {c_splitter.split_text(text)}\\n\\n\")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator = ' '\n",
    ")\n",
    "print(f\"Character splitter using ' ' separator:\\n {c_splitter.split_text(text)}\")"
   ],
   "id": "1323cf858037d995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character splitter using default separator:\n",
      " ['Today is Tuesday and we are in Natural Language Processing class.']\n",
      "\n",
      "\n",
      "Character splitter using ' ' separator:\n",
      " ['Today is Tuesday and we are in', 'we are in Natural Language', 'Language Processing class.']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Recursive splitting\n",
    "In the following examples we compare the work of `CharacterTextSplitter` and `RecursiveCharacterTextSplitter`.  \n",
    "\n",
    "`RecursiveCharacterTextSplitter` gives better results since it separates by separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", ‚Äú‚Äù], which means that it will split the text by \"\\n\\n\" first, and if the chunk length is still too big, it will split based on the next separator from the separators array.  \n",
    "\n",
    "Take a look at the last two chunks from the splitters results and see how recursive one did better job keeping paragraphs together."
   ],
   "id": "5b4ffa316ae412f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:05:49.894663Z",
     "start_time": "2024-10-27T10:05:49.886292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text2 = \"\"\"ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch.\n",
    "\n",
    "These models support common tasks in different modalities, such as:\n",
    "üìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.\n",
    "üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.\n",
    "üó£Ô∏è Audio: automatic speech recognition and audio classification.\n",
    "üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n",
    "\n",
    "ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\"\"\""
   ],
   "id": "bff8930e79562f23",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:05:52.387134Z",
     "start_time": "2024-10-27T10:05:52.379968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=550,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "print(\"[CHUNK]  \" + \"\\n\\n[CHUNK]  \".join(c_splitter.split_text(text2)))"
   ],
   "id": "38e23feeaa41dfab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK]  ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch.\n",
      "\n",
      "These models support common tasks in different modalities, such as:\n",
      "üìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.\n",
      "üñºÔ∏è Computer Vision: image classification,\n",
      "\n",
      "[CHUNK]  object detection, and segmentation.\n",
      "üó£Ô∏è Audio: automatic speech recognition and audio classification.\n",
      "üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n",
      "\n",
      "ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another.\n",
      "\n",
      "[CHUNK]  Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:05:55.791628Z",
     "start_time": "2024-10-27T10:05:55.784269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=550,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "print(\"[CHUNK]  \" + \"\\n\\n[CHUNK]  \".join(r_splitter.split_text(text2)))"
   ],
   "id": "145b3ef4bf02dfd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK]  ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch.\n",
      "\n",
      "[CHUNK]  These models support common tasks in different modalities, such as:\n",
      "üìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.\n",
      "üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.\n",
      "üó£Ô∏è Audio: automatic speech recognition and audio classification.\n",
      "\n",
      "[CHUNK]  üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n",
      "\n",
      "[CHUNK]  ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can split pages form the loaded pdf file into documents using RecursiveCharacterTextSplitter.",
   "id": "f51c6232fabbd869"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:06:00.215272Z",
     "start_time": "2024-10-27T10:05:59.134370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"knowledge_base/LinearRegression-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "docs = r_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{len(pages)} pages were split in {len(docs)} documents.\")\n",
    "# for doc in docs:\n",
    "#     print(f\"[DOC] {doc.page_content} \\n\")"
   ],
   "id": "ea2a4fd7126c5482",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 pages were split in 47 documents.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Token splitting\n",
    "Now we can split on token count instead of characters count. This practice is important because LLMs often have context windows designated in tokens."
   ],
   "id": "648038909491834d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:06:03.998257Z",
     "start_time": "2024-10-27T10:06:03.789951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "print(text_splitter.split_text(text))\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)"
   ],
   "id": "a4283a36a985221e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', ' is', ' Tuesday', ' and', ' we', ' are', ' in', ' Natural', ' Language', ' Processing', ' class', '.']\n",
      "{'source': 'knowledge_base/LinearRegression-Lecture01.pdf', 'page': 0}\n",
      "3. Linear regression\n",
      "Machine Learning 1, UNIZG FER, AY 2022/2023\n",
      "Jan ≈†najder, lectures, v1.13\n",
      "Last time we introduced the basic concepts of machine learning: the hypothesis , which is a\n",
      "function that maps from input data to labels and is deÔ¨Åned by parameters theta, and the model,\n",
      "which is a set of hypotheses indexed by parameters theta. We have said that machine learning\n",
      "comes down\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Context aware splitting\n",
    "    \n",
    "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks. Specify headers to split on."
   ],
   "id": "7096961c45d217f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:06:22.611802Z",
     "start_time": "2024-10-27T10:06:22.575374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_document = \"\"\"# NLP\\n\\n \\\n",
    "## What is NLP\\n\\n \\\n",
    "NLP is a field at the intersection of computer science, artificial intelligence (AI), and linguistics.\\n\\n It is also a subject on FER.\\n\\n \\\n",
    "### What is that name? \\n\\n \\\n",
    "Natural Language Processing \\n\\n \n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "md_header_splits[0]"
   ],
   "id": "9bde562986182467",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'NLP', 'Header 2': 'What is NLP'}, page_content='NLP is a field at the intersection of computer science, artificial intelligence (AI), and linguistics.  \\nIt is also a subject on FER.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now chunks have to be moved in vector stores; we have to store them with indexing so they are easily retrieved when asking questions.\n",
    "\n",
    "### Vectorstores and Embedding\n",
    "Embeddings take a peace of text and create a numerical vector representation of that text. Text with similar content will have similar vectors.  \n",
    "We can save data as embedded vectors in vector store. Later we can also make embedding vector from our question, and based on that vector, we can query all the similar vectors from vector store. Those vectors will be passed inside LLM, and it will produce the answer.   \n",
    "\n",
    "Now we are going to load a few PDF files, and we will duplicate the first one to create messy data. Then, we will split loaded data into chunks."
   ],
   "id": "6f1ef8c458a06fa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:53:28.132389Z",
     "start_time": "2024-10-27T14:53:26.320407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load PDFs\n",
    "loaders = [\n",
    "    PyPDFLoader(\"knowledge_base/LinearRegression-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"knowledge_base/LinearRegression-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"knowledge_base/LinearRegression-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"knowledge_base/LinearDiscriminativeModels-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "  \n",
    "# split data  \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ],
   "id": "ee6246bdd267eb8c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Simple example for understanding embeddings\n",
    "We are using <b>sentence-transformers</b>, open source library for generating embeddings.  \n",
    "Sentence-transformers is built on top of <b>Hugging Face Transformers</b> and provides various pre-trained models for generating embeddings."
   ],
   "id": "341edcc93b80fefa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:53:47.119903Z",
     "start_time": "2024-10-27T14:53:44.217137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "sentence1 = \"I like piano\"\n",
    "sentence2 = \"I like guitar\"\n",
    "sentence3 = \"Entschuldigung, wo ist der Kellner?\"\n",
    "\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)\n",
    "\n",
    "print(f\"E1 vs E2: {np.dot(embedding1, embedding2)}\")\n",
    "print(f\"E1 vs E3: {np.dot(embedding1, embedding3)}\")\n",
    "print(f\"E2 vs E3: {np.dot(embedding2, embedding3)}\")"
   ],
   "id": "5252861b46ffcc30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 vs E2: 0.7652282697764397\n",
      "E1 vs E3: -0.00959956912924178\n",
      "E2 vs E3: -0.003178931187782787\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Vectorstores\n",
    "LangChain has integrations with a lot of vector stores, and we are using Chroma. It is lightweight and in-memory.  \n",
    "On vector DB we can call `similarity_search` method and pass it parameter p for how many results we want. So, for our question `similarity_search` will return k answers from vector storage based on embedding vectors similarities.\n"
   ],
   "id": "20c9c372233764c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:54:01.548652Z",
     "start_time": "2024-10-27T14:53:50.771421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'chroma/'\n",
    "    \n",
    "!rm -rf ./chroma  # remove old database files if any\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ],
   "id": "9df94dc3119652b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T09:57:20.831620Z",
     "start_time": "2024-10-27T09:57:19.896934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"what is Least-squares method\"\n",
    "docs = vectordb.similarity_search(question,k=2)\n",
    "for doc in docs:\n",
    "    print(f\"{doc.page_content[:1000]}\\n\\n\")"
   ],
   "id": "7a8686f69d29e816",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Least-squares method\n",
      "Let us now consider in more detail the least-squares method, that is, how to obtain the the\n",
      "parameters wthat are optimal in terms of the sum of squared residuals, an let‚Äôs do this for\n",
      "multiple linear regression, where nƒÖ1. We have a set of examples (in regression, these are\n",
      "input variables or ‚Äúindependent variables‚Äù):\n",
      "X‚Äú¬®\n",
      "ÀöÀöÀöÀöÀù1xp1q\n",
      "1xp1q\n",
      "2... xp1q\n",
      "n\n",
      "1xp2q\n",
      "1xp2q\n",
      "2... xp2q\n",
      "n\n",
      "...\n",
      "1xpNq\n",
      "1xpNq\n",
      "2... xpNq\n",
      "nÀõ\n",
      "‚Äπ‚Äπ‚Äπ‚Äπ‚Äö\n",
      "NÀÜpn`1q\n",
      "Recall that the matrix Xis called the design matrix . We also have a vector of output values\n",
      "(i.e., the labels or ‚Äúdependent variables‚Äù):\n",
      "y‚Äú¬®\n",
      "ÀöÀöÀöÀùyp1q\n",
      "yp2q\n",
      "...\n",
      "ypNqÀõ\n",
      "‚Äπ‚Äπ‚Äπ‚Äö\n",
      "NÀÜ1\n",
      "4.1 A dead end: Exact solution\n",
      "Let‚Äôs try Ô¨Årst with something other than the least-squares method, and that will turn out to be\n",
      "a bad idea. Namely, we can think of our optimization problem as a system of equations. Ideally,\n",
      "we would like to Ô¨Ånd a solution for which\n",
      "pxpiq,ypiqqPD. hpxpiqq‚Äúypiq\n",
      "that is\n",
      "pxpiq,ypiqqPD.wTxpiq‚Äúypiq\n",
      "This would be an exact solution of our problem: a solut\n",
      "\n",
      "\n",
      "4 Least-squares method\n",
      "Let us now consider in more detail the least-squares method, that is, how to obtain the the\n",
      "parameters wthat are optimal in terms of the sum of squared residuals, an let‚Äôs do this for\n",
      "multiple linear regression, where nƒÖ1. We have a set of examples (in regression, these are\n",
      "input variables or ‚Äúindependent variables‚Äù):\n",
      "X‚Äú¬®\n",
      "ÀöÀöÀöÀöÀù1xp1q\n",
      "1xp1q\n",
      "2... xp1q\n",
      "n\n",
      "1xp2q\n",
      "1xp2q\n",
      "2... xp2q\n",
      "n\n",
      "...\n",
      "1xpNq\n",
      "1xpNq\n",
      "2... xpNq\n",
      "nÀõ\n",
      "‚Äπ‚Äπ‚Äπ‚Äπ‚Äö\n",
      "NÀÜpn`1q\n",
      "Recall that the matrix Xis called the design matrix . We also have a vector of output values\n",
      "(i.e., the labels or ‚Äúdependent variables‚Äù):\n",
      "y‚Äú¬®\n",
      "ÀöÀöÀöÀùyp1q\n",
      "yp2q\n",
      "...\n",
      "ypNqÀõ\n",
      "‚Äπ‚Äπ‚Äπ‚Äö\n",
      "NÀÜ1\n",
      "4.1 A dead end: Exact solution\n",
      "Let‚Äôs try Ô¨Årst with something other than the least-squares method, and that will turn out to be\n",
      "a bad idea. Namely, we can think of our optimization problem as a system of equations. Ideally,\n",
      "we would like to Ô¨Ånd a solution for which\n",
      "pxpiq,ypiqqPD. hpxpiqq‚Äúypiq\n",
      "that is\n",
      "pxpiq,ypiqqPD.wTxpiq‚Äúypiq\n",
      "This would be an exact solution of our problem: a solut\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can notice that two resulting docs are the same, and there is no real value in the second one. (This is result of loading the same pdf file two times, and now we have chunks with the same content.)",
   "id": "81e5d78f20e77d94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Retrieval\n",
    "A query comes in and we have to retrieve most relevant data (chunks) for that query. \n",
    "\n",
    "#### Method: Maximum marginal relevance MMR\n",
    "- You may not always want to choose the most similar responses - need of diverse set of documents \n",
    "- Idea is to query vector store with fetch_k param that we can control \n",
    "- Choose <b>fetch_k</b> most similar responses based on semantics \n",
    "- Within those fetch_k responses choose final <b>k</b> most diverse responses for user\n",
    "- With <b>lambda_mult</b> parameter determine the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5."
   ],
   "id": "b48d1c4d331167f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T20:40:57.622746Z",
     "start_time": "2024-10-26T20:40:57.149421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question, fetch_k=5, k=2)\n",
    "for doc in docs_mmr:\n",
    "    print(f\"{doc.page_content[:1000]}\\n\\n\")"
   ],
   "id": "77686eb400621f8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Least-squares method\n",
      "Let us now consider in more detail the least-squares method, that is, how to obtain the the\n",
      "parameters wthat are optimal in terms of the sum of squared residuals, an let‚Äôs do this for\n",
      "multiple linear regression, where nƒÖ1. We have a set of examples (in regression, these are\n",
      "input variables or ‚Äúindependent variables‚Äù):\n",
      "X‚Äú¬®\n",
      "ÀöÀöÀöÀöÀù1xp1q\n",
      "1xp1q\n",
      "2... xp1q\n",
      "n\n",
      "1xp2q\n",
      "1xp2q\n",
      "2... xp2q\n",
      "n\n",
      "...\n",
      "1xpNq\n",
      "1xpNq\n",
      "2... xpNq\n",
      "nÀõ\n",
      "‚Äπ‚Äπ‚Äπ‚Äπ‚Äö\n",
      "NÀÜpn`1q\n",
      "Recall that the matrix Xis called the design matrix . We also have a vector of output values\n",
      "(i.e., the labels or ‚Äúdependent variables‚Äù):\n",
      "y‚Äú¬®\n",
      "ÀöÀöÀöÀùyp1q\n",
      "yp2q\n",
      "...\n",
      "ypNqÀõ\n",
      "‚Äπ‚Äπ‚Äπ‚Äö\n",
      "NÀÜ1\n",
      "4.1 A dead end: Exact solution\n",
      "Let‚Äôs try Ô¨Årst with something other than the least-squares method, and that will turn out to be\n",
      "a bad idea. Namely, we can think of our optimization problem as a system of equations. Ideally,\n",
      "we would like to Ô¨Ånd a solution for which\n",
      "pxpiq,ypiqqPD. hpxpiqq‚Äúypiq\n",
      "that is\n",
      "pxpiq,ypiqqPD.wTxpiq‚Äúypiq\n",
      "This would be an exact solution of our problem: a solut\n",
      "\n",
      "\n",
      "is the so-called pseudoinverse (more precisely: the Moore-Penrose inverse) of matrix X. What\n",
      "is a pseudoinverse? Pseudoinverse is a generalization of the concept of matrix inverse. Unlike\n",
      "ordinary inverse, which exists only for full-rank square matrices, pseudoinverse exists for just\n",
      "about every matrix . As a special case, if the matrix Xis square and of full rank, then the\n",
      "pseudoinverse is equal to the ordinary inverse, X`‚ÄúX¬¥1.\n",
      "What is important to note here is that the pseudoinverse is in and of itself the solution to\n",
      "least squares problem. Namely, the initially deÔ¨Åned the problem in terms of squared diÔ¨Äerences,\n",
      "i.e., the quadratic error function was built into the problem from the onset. This means that\n",
      "calculating the pseudoinverse will give us the parameters wfor which the hyperplane deÔ¨Åned by\n",
      "hpx;wqis optimal in terms of least square diÔ¨Äerences from labeled data.\n",
      "More precisely, the solution wgiven by this equation is such that the distance in terms of\n",
      "squared diÔ¨Äerences between \n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can notice that now the results are more diverse, we did not get two of the same docs, but two separate ones which means we got more info than before.\n",
    "\n",
    "### Method: SelfQuery - LLM aided retrieval\n",
    "- We often want to infer the metadata from the query itself\n",
    "- We can use `SelfQueryRetriever`, which uses an LLM to extract:  \n",
    "  - The query string to use for vector search\n",
    "  - A metadata filter to pass in as well\n",
    "- Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
   ],
   "id": "2d335a07bada16db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:56:23.847382Z",
     "start_time": "2024-10-27T15:56:23.822074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# Define metadata field information for retrieval\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of knowledge_base/LinearRegression-Lecture01.pdf, knowledge_base/LinearRegression-Lecture02.pdf, or knowledge_base/LinearDiscriminativeModels-Lecture03.pdf\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Lecture script\"\n"
   ],
   "id": "a1e24f312538bb9",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:14:11.012184Z",
     "start_time": "2024-10-27T16:14:10.954451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]  "
   ],
   "id": "6bc7171b540aebd4",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:14:13.739760Z",
     "start_time": "2024-10-27T16:14:13.441605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    ")  "
   ],
   "id": "f31a05e9f755e37b",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:14:34.915647Z",
     "start_time": "2024-10-27T16:14:19.380923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")\n",
    "question = \"what did they say about regression in the second lecture?\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "for d in docs:\n",
    "    print(d.metadata)"
   ],
   "id": "d0ab66918ad552c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n",
      "{'page': 1, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n",
      "{'page': 5, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n",
      "{'page': 3, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We used an open-source <b>Hugging Face LLM model meta-llama/Meta-Llama-3-8B-Instruct</b>.  \n",
    "  \n",
    "Results are documents only from the second lecture, just like we specified in the query which means that retriever retrieved documents based on document content and <b>document metadata</b> also."
   ],
   "id": "1ba849a317eec6fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:24:21.354057Z",
     "start_time": "2024-10-27T16:24:21.168260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAI\n",
    "load_dotenv()\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "llm = OpenAI()"
   ],
   "id": "fa55ac4836e33dfa",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:24:35.458726Z",
     "start_time": "2024-10-27T16:24:29.291517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")\n",
    "question = \"what did they say about regression in the second lecture?\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "for d in docs:\n",
    "    print(d.metadata)"
   ],
   "id": "62228c930a820e23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n",
      "{'page': 1, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n",
      "{'page': 5, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n",
      "{'page': 3, 'source': 'knowledge_base/LinearRegression-Lecture02.pdf'}\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We got the same result using OpenAI LLM.",
   "id": "b71b5a98821bebd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Method: Compression  \n",
    "- To pull out only the most relevant parts of responses \n",
    "- With compression, we run all the documents through the compression LLM and extract most relevant segments which will be passed to the main LLM that will generate the answer\n",
    "- More LLM calls but finding the answer with focus on the most relevant things  \n",
    "\n",
    "We are using compressor with open source Hugging Face LLM meta-llama/Meta-Llama-3-8B-Instruct."
   ],
   "id": "2db233f5c37f4479"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T14:59:06.647820Z",
     "start_time": "2024-10-27T14:59:06.630149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]          "
   ],
   "id": "27b4fd22be035a0b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:27:12.215383Z",
     "start_time": "2024-10-27T16:27:00.746778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# models tried: tiiuae/falcon-7b-instruct, meta-llama/Meta-Llama-3-8B-Instruct\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    ") \n",
    "llm.invoke(\"what was the first disney movie?\") "
   ],
   "id": "7d46b0a3222431fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Disney movie was Snow White and the Seven Dwarfs, which was released in 1937. It was the first full-length animated feature film produced by Walt Disney Productions and was based on the classic fairy tale \"Snow White\" by the Brothers Grimm. The movie was a groundbreaking achievement in animation and storytelling, and it became a huge success, earning critical acclaim and breaking box office records. It was also the first Disney movie to be released in theaters, and it helped establish Walt Disney as a major player in the film industry.... Read more\\nHow many Disney movies have been made? As of 2022, there have been over 60 animated feature films produced by Walt Disney Animation Studios, including Snow White and the Seven Dwarfs (1937), Pinocchio (1940), Fantasia (1940), Dumbo (1941), Bambi (1942), Cinderella (1950), Mary Poppins (1964), The Jungle Book (1967), The Little Mermaid (1989), Beauty and the Beast (1991), Aladdin (1992), The Lion King (1994), and many others. Additionally, there have been numerous Disney live-action films, Pixar films, Marvel films, and Star Wars films produced by Disney over the years.... Read more\\nWhat is the most popular Disney movie? According to various sources, including Box Office Mojo and the Internet Movie Database (IMDb), the most popular Disney movie is Snow White and the Seven Dwarfs (1937). It is the highest-grossing film of all time, adjusted for inflation, and has been named the greatest film of all time by the American Film Institute. It has also been selected for preservation in the National Film Registry by the Library of Congress. Other popular Disney movies include Bambi (1942), Cinderella (1950), Mary Poppins (1964), The Jungle Book (1967), The Little Mermaid (1989), Beauty and the Beast (1991), and The Lion King (1994).... Read more\\nWhat is the most profitable Disney movie? According to Box Office Mojo, the most profitable Disney movie is Frozen (2013), which grossed over $1.27 billion worldwide. This includes theatrical re-releases and home video sales. Frozen is also the highest-grossing animated film of all time, and it won two Academy Awards, including Best Animated Feature. Other profitable Disney movies include The Lion King (1994), Beauty and the Beast (1991), The'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:27:40.937314Z",
     "start_time": "2024-10-27T16:27:23.440759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"What is Least-squares method?\"\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ],
   "id": "6edf49c7db174fea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "* The pseudoinverse is the solution to the least squares problem.\n",
      "* Calculating the pseudoinverse gives the parameters w for which the hyperplane is optimal in terms of least square differences from labeled data.\n",
      "* The solution w is such that the distance in terms of squared differences between vectors Xw and y is minimal, that is, it is a solution minimizing the L2-norm.\n",
      "* If the system of equations is underdetermined and has multiple solutions, the pseudoinverse gives the solution with the smallest norm.\n",
      "* If the system is overdetermined, the pseudoinverse gives a solution that minimizes }Xw¬¥y}, but if such a solution is not unique, it again gives the one with the smallest norm.\n",
      "* The optimization procedure gives a closed-form solution, but obtaining this solution can still be quite computationally demanding.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "* The pseudoinverse is the solution to the least squares problem.\n",
      "* Calculating the pseudoinverse gives the parameters w for which the hyperplane is optimal in terms of least square differences from labeled data.\n",
      "* The solution w is such that the distance in terms of squared differences between vectors Xw and y is minimal, that is, it is a solution minimizing the L2-norm.\n",
      "* If the system of equations is underdetermined and has multiple solutions, the pseudoinverse gives the solution with the smallest norm.\n",
      "* If the system is overdetermined, the pseudoinverse gives a solution that minimizes }Xw¬¥y}, but if such a solution is not unique, it again gives the one with the smallest norm.\n",
      "* The optimization procedure gives a closed-form solution, but obtaining this solution can still be quite computationally demanding.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "4 Least-squares method \n",
      "Let us now consider in more detail the least-squares method, that is, how to obtain the parameters wthat are optimal in terms of the sum of squared residuals, and let‚Äôs do this for multiple linear regression, where nƒÖ1.\n",
      "\n",
      "Answer: \n",
      "The Least-squares method is a method to obtain the parameters (w) that are optimal in terms of the sum of squared residuals in multiple linear regression. It is used to find the best fit of a linear model to a set of data. The goal is to minimize the sum of the squared differences between the observed values and the predicted values. The method involves finding the values of w that minimize the sum of the squared residuals. The extracted context mentions that the least-squares method is used to obtain the optimal parameters w in terms of the sum of squared residuals in multiple linear regression. It also mentions that the method involves finding the values of w that minimize the sum of the squared residuals. Therefore, the answer is: The Least-squares method is a method to obtain the parameters (w) that are optimal in terms of the sum of squared residuals in multiple linear regression. It is used to find the best fit of a linear model to a set of data. The goal is to minimize the sum of the squared differences between the observed values and the predicted values. The method involves finding the values of w that minimize the sum of the squared residuals.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "4 Least-squares method \n",
      "Let us now consider in more detail the least-squares method, that is, how to obtain the parameters wthat are optimal in terms of the sum of squared residuals, and let‚Äôs do this for multiple linear regression, where nƒÖ1.\n",
      "\n",
      "Answer: \n",
      "The Least-squares method is a method to obtain the parameters (w) that are optimal in terms of the sum of squared residuals in multiple linear regression. It is used to find the best fit of a linear model to a set of data. The goal is to minimize the sum of the squared differences between the observed values and the predicted values. The method involves finding the values of w that minimize the sum of the squared residuals. The extracted context mentions that the least-squares method is used to obtain the optimal parameters w in terms of the sum of squared residuals in multiple linear regression. It also mentions that the method involves finding the values of w that minimize the sum of the squared residuals. Therefore, the answer is: The Least-squares method is a method to obtain the parameters (w) that are optimal in terms of the sum of squared residuals in multiple linear regression. It is used to find the best fit of a linear model to a set of data. The goal is to minimize the sum of the squared differences between the observed values and the predicted values. The method involves finding the values of w that minimize the sum of the squared residuals.\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:28:13.907277Z",
     "start_time": "2024-10-27T16:28:13.897419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_contexts_len = len(\"\\n\\n\".join([d.page_content for i, d in enumerate(docs)]))\n",
    "compressed_contexts_len = len(\"\\n\\n\".join([d.page_content for i, d in enumerate(compressed_docs)]))\n",
    "\n",
    "print(\"Original context length:\", original_contexts_len)\n",
    "print(\"Compressed context length:\", compressed_contexts_len)\n",
    "print(\"Compressed Ratio:\", f\"{original_contexts_len/(compressed_contexts_len + 1e-5):.2f}x\")"
   ],
   "id": "61efff30623c952f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original context length: 6413\n",
      "Compressed context length: 4500\n",
      "Compressed Ratio: 1.43x\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that documents are duplicated (in the background semantic similarity is being used), but LLM extracted only the parts relevant to the question, and did not return whole documents.\n",
    "\n",
    "### Question answering\n",
    "RetrievalQA chain is doing question answering backed by a retrieval step.  \n",
    "We are using same open source LLM as before."
   ],
   "id": "14bd569e23bd9aff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:38:02.886948Z",
     "start_time": "2024-10-27T16:37:59.338531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "result = qa_chain({\"query\": \"What is Least-squares method?\"})\n",
    "result[\"result\"]"
   ],
   "id": "7888b67994560050",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The least-squares method is a method used in linear regression to find the best-fitting line that minimizes the sum of the squared errors between the observed and predicted values. It is a way to solve the problem of overfitting by using a closed-form solution.\\n\\nCorrect Answer: The least-squares method is a method used in linear regression to find the best-fitting line that minimizes the sum of the squared errors between the observed and predicted values. It is a way to solve the problem of overfitting by using a closed-form solution.\\n\\nFinal Answer: The least-squares method is a method used in linear regression to find the best-fitting line that minimizes the sum of the squared errors between the observed and predicted values. It is a way to solve the problem of overfitting by using a closed-form solution.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Map-reduce method & RetrievalQA\n",
    " - Each document is sent to LLM model by itself to get the answer, and those answers are composed into one answer with the final call to the LLM\n",
    " - The con is many LLM calls - slow\n",
    " - May be worse answer if the info is spread over more documents and it only looks at documents individually "
   ],
   "id": "525ee73d750936"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:46:32.094988Z",
     "start_time": "2024-10-27T16:46:17.526485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ],
   "id": "fd206feaecf68465",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know. The provided content does not discuss the Least-squares method. It seems to be a passage about a president's speech, and has no relation to the topic of Least-squares method.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Results are not that great because many documents did not have the answer and the answer was lost.",
   "id": "82b86d131d3d6570"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Refine  \n",
    "  - One LLM call starts a sequential LLM calls and every new call is searching for the better answer based on the answer from the previous LLM and new document from the documents list\n",
    "  - Carry over the info between the documents "
   ],
   "id": "be96814bfbc9ddb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:49:03.037820Z",
     "start_time": "2024-10-27T16:48:37.774545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ],
   "id": "8538e1009a1411a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSince the original answer was not very detailed, this answer can be refined to provide more context and details about the least-squares method. \\n\\nThe least-squares method is a mathematical technique used in regression analysis to find the best-fitting linear or nonlinear model for a set of data points. In the context of multiple linear regression, the goal is to find the optimal values of the regression coefficients (w1, w2, ‚Ä¶, wn) that minimize the sum of the squared residuals between the observed values of the dependent variable (y) and the predicted values of the dependent variable based on the linear combination of the independent variables (X).\\n\\nThe least-squares method is often used in statistical modeling to estimate the parameters of a linear regression model. The process involves the following steps:\\n\\n1. Collect a dataset of input variables (X) and output variables (y).\\n2. Define a linear regression model that relates the input variables to the output variable.\\n3. Use the least-squares method to find the optimal values of the regression coefficients that minimize the sum of the squared residuals.\\n\\nThe least-squares method can be implemented using various techniques, such as:\\n\\n1. Ordinary least squares (OLS): This method involves minimizing the sum of the squared residuals using an iterative process.\\n2. Total least squares (TLS): This method involves minimizing the sum of the squared residuals and the sum of the squared errors in the regression coefficients.\\n3. Ridge regression: This method involves adding a penalty term to the sum of the squared residuals to prevent overfitting.\\n\\nIn summary, the least-squares method is a powerful technique used in regression analysis to find the best-fitting linear or nonlinear model for a set of data points. It is widely used in statistical modeling to estimate the parameters of a linear regression model and is an essential tool in data analysis and machine learning.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We got better result this time, the info was carried over between the documents.",
   "id": "69f481bf549b6d79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat   \n",
    "Add a concept of chat history.  \n",
    "ConversationBufferMemory will keep a list of chat messages in history, and it will pass those along with a question to a chatbot.\n"
   ],
   "id": "472b6763de736096"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T17:00:17.888621Z",
     "start_time": "2024-10-27T17:00:17.879591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ],
   "id": "59caf3c6d5a7f31b",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T17:03:17.867081Z",
     "start_time": "2024-10-27T17:03:17.436810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ],
   "id": "4c0204fd138c78d6",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T17:04:38.509018Z",
     "start_time": "2024-10-27T17:04:30.702774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"What is Least-squares method?\" # first question\n",
    "result = qa({\"question\": question})"
   ],
   "id": "3cd15df088536d8c",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T17:04:46.347466Z",
     "start_time": "2024-10-27T17:04:46.281671Z"
    }
   },
   "cell_type": "code",
   "source": "result['answer'] ",
   "id": "dafa1b79ce45c0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The least-squares method is a method used in linear regression to find the best-fitting line that minimizes the sum of the squared errors between the observed and predicted values. It is a way to solve the problem of overfitting by using a closed-form solution.\\n\\nCorrect Answer: The least-squares method is a method used in linear regression to find the best-fitting line that minimizes the sum of the squared errors between the observed and predicted values. It is a way to solve the problem of overfitting by using a closed-form solution.\\n\\nFinal Answer: The least-squares method is a method used in linear regression to find the best-fitting line that minimizes the sum of the squared errors between the observed and predicted values. It is a way to solve the problem of overfitting by using a closed-form solution.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T17:06:31.641952Z",
     "start_time": "2024-10-27T17:06:20.008041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"why is that method needed?\"  # follow-up question\n",
    "result = qa({\"question\": question})"
   ],
   "id": "f5f9eb3a76cdf52f",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T17:06:48.250641Z",
     "start_time": "2024-10-27T17:06:48.222917Z"
    }
   },
   "cell_type": "code",
   "source": "result['answer'] ",
   "id": "4c120ada32f536ea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The least-squares method is needed because the exact solution is not always possible due to the limitations of real-world data. In the examples provided, it is shown that the exact solution can fail when the design matrix is not square or when there is noise in the data. The least-squares method is a way to find a solution that is optimal in terms of the sum of squared residuals, even when the exact solution is not possible. It is a more robust and practical approach to solving regression problems.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We got a nice answer on our follow-up question. Created ChatBot keeps track of the history of the conversation, and it knew on which method we are referencing and gave us the right answer. ",
   "id": "1d44bcc7e4b48789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3cf4df437b5012c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
